{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f738ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34387f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'youtube_transcript_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myoutube_transcript_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YouTubeTranscriptApi\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myoutube_transcript_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformatters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextFormatter\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myoutube_transcript_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformatters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextFormatter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'youtube_transcript_api'"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "import re\n",
    "\n",
    "def get_video_id(url):\n",
    "    match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def fetch_available_captions(video_url):\n",
    "    video_id = get_video_id(video_url)\n",
    "    if not video_id:\n",
    "        print(\"Invalid YouTube URL.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Get list of available transcripts\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "\n",
    "        # Try manually created transcript first, then fallback to generated\n",
    "        try:\n",
    "            transcript = transcript_list.find_manually_created_transcript(transcript_list._manually_created_transcripts.keys())\n",
    "        except:\n",
    "            transcript = transcript_list.find_generated_transcript(transcript_list._generated_transcripts.keys())\n",
    "\n",
    "        # Format the transcript as plain text\n",
    "        formatter = TextFormatter()\n",
    "        text = formatter.format_transcript(transcript.fetch())\n",
    "\n",
    "        print(f\"\\n--- Captions (Language: {transcript.language_code}) ---\\n\")\n",
    "        print(text)\n",
    "        return text\n",
    "        # print(len(text))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "# Example usage\n",
    "url = input(\"Enter YouTube video URL: \")\n",
    "text= fetch_available_captions(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Your retrieved caption text\n",
    "text = text  # Replace with the text you retrieved from YouTube captions\n",
    "\n",
    "# 2. Initialize LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# AIzaSyA3GbDc39XAxR-4fVHII3D0mf_5Ftf7ph8\n",
    "# 3. Wrap caption text as Document\n",
    "doc = Document(page_content=text)\n",
    "\n",
    "# 4. Split text\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "docs = splitter.split_documents([doc])\n",
    "\n",
    "# 5. Custom prompts\n",
    "map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a podcast summarizer.\n",
    "Summarize the following transcript text in **short, clear, bullet points**.\n",
    "Write just in 5-7 points.\n",
    "\n",
    "Transcript:\n",
    "{text}\n",
    "\n",
    "Bullet Point Summary:\n",
    "\"\"\")\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert summarizer.\n",
    "Given these bullet point summaries of a podcast transcript, merge them into a **final short list**.\n",
    "Write in **bullet points**. No more than 7 t0 8  points total it should be just 10 line.\n",
    "\n",
    "Partial Summaries:\n",
    "{text}\n",
    "\n",
    "Final Bullet Point Summary:\n",
    "\"\"\")\n",
    "\n",
    "# 6. Load summarization chain\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=reduce_prompt\n",
    ")\n",
    "\n",
    "# 7. Run summarization\n",
    "summary = chain.invoke(docs)\n",
    "print(summary['output_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}